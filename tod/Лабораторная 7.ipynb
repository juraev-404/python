{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Введение в обработку текста на естественном языке"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Материалы:\n",
    "\n",
    "* https://realpython.com/nltk-nlp-python/\n",
    "* https://scikit-learn.org/stable/modules/feature_extraction.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Разминка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Считайте слова из файла `litw-win.txt` и запишите их в список `words`. В заданном предложении исправьте все опечатки, заменив слова с опечатками на ближайшие (в смысле расстояния Левенштейна) к ним слова из списка `words`. Считайте, что в слове есть опечатка, если данное слово не содержится в списке `words`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''с велечайшим усилием выбравшись из потока убегающих людей Кутузов со свитой уменьшевшейся вдвое поехал на звуки выстрелов русских орудий''' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Разбейте текст из формулировки задания 1 на слова; проведите стемминг и лемматизацию слов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Преобразуйте предложения из формулировки задания 1 в векторы при помощи `CountVectorizer`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Лабораторная работа 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Расстояние редактирования"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import random\n",
    "from nltk.metrics import edit_distance\n",
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
    "from Levenshtein import distance as levenshtein_distance\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.spatial.distance import cosine\n",
    "from itertools import combinations\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1 Загрузите предобработанные описания рецептов из файла `preprocessed_descriptions.csv`. Получите набор уникальных слов `words`, содержащихся в текстах описаний рецептов (воспользуйтесь `word_tokenize` из `nltk`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\khaiy/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество уникальных слов: 29764\n",
      "['!', '#', '$', '%', '&', \"'\", \"''\", \"'00\", \"'03\", \"'04\", \"'05\", \"'06\", \"'07\", \"'08\", \"'09\", \"'10\", \"'1001\", \"'12\", \"'300\", \"'333\"]\n"
     ]
    }
   ],
   "source": [
    "# Скачиваем токенизатор\n",
    "nltk.download('punkt')\n",
    "\n",
    "df = pd.read_csv(\"preprocessed_descriptions.csv\")\n",
    "\n",
    "# Объединяем все описания в один текст\n",
    "all_text = \" \".join(df['description'].astype(str))\n",
    "\n",
    "# Токенизация\n",
    "tokens = word_tokenize(all_text)\n",
    "\n",
    "# Множество уникальных слов\n",
    "words = set(tokens)\n",
    "\n",
    "print(f\"Количество уникальных слов: {len(words)}\")\n",
    "print(sorted(list(words))[:20])  # первые 20 слов\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2 Сгенерируйте 5 пар случайно выбранных слов и посчитайте между ними расстояние редактирования."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "guestimate — closer → расстояние редактирования: 9\n",
      "lunck — zea → расстояние редактирования: 5\n",
      "volunteer — dedos → расстояние редактирования: 9\n",
      "chlorophyll — saltscapes → расстояние редактирования: 10\n",
      "cheesier — contessa → расстояние редактирования: 6\n"
     ]
    }
   ],
   "source": [
    "# Преобразуем во множество без знаков препинания\n",
    "filtered_words = [w for w in words if w.isalpha()]\n",
    "\n",
    "# Случайно выбираем 5 пар\n",
    "sample_words = random.sample(filtered_words, 10)\n",
    "\n",
    "# Группируем в пары\n",
    "pairs = list(zip(sample_words[::2], sample_words[1::2]))\n",
    "\n",
    "# Вычисляем расстояния\n",
    "for w1, w2 in pairs:\n",
    "    dist = edit_distance(w1, w2)\n",
    "    print(f\"{w1} — {w2} → расстояние редактирования: {dist}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3 Напишите функцию, которая для заданного слова `word` возвращает `k` ближайших к нему слов из списка `words` (близость слов измеряется с помощью расстояния Левенштейна)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['flair', 'fair', 'affair', 'air', 'blais']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def k_nearest_words(word, words, k):\n",
    "    # Вычисляем расстояния Левенштейна от word до каждого слова из списка\n",
    "    distances = [(levenshtein_distance(word, w), w) for w in words]\n",
    "    \n",
    "    # Находим k слов с наименьшими расстояниями\n",
    "    closest = sorted(distances)[0:k]\n",
    "    \n",
    "    # Возвращаем только слова (без расстояний)\n",
    "    return [w for _, w in closest]\n",
    "k_nearest_words('flair', words, 5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Стемминг, лемматизация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1 На основе результатов 1.1 создайте `pd.DataFrame` со столбцами: \n",
    "    * word\n",
    "    * stemmed_word \n",
    "    * normalized_word \n",
    "\n",
    "Столбец `word` укажите в качестве индекса. \n",
    "\n",
    "Для стемминга воспользуйтесь `SnowballStemmer`, для нормализации слов - `WordNetLemmatizer`. Сравните результаты стемминга и лемматизации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\khaiy/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\khaiy/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          stemmed_word normalized_word\n",
      "word                                  \n",
      "imbergamo    imbergamo       imbergamo\n",
      "vibrance       vibranc        vibrance\n",
      "granita        granita         granita\n",
      "tinkered        tinker        tinkered\n",
      "'same-old     same-old       'same-old\n",
      "...                ...             ...\n",
      "sail              sail            sail\n",
      "calories        calori         calorie\n",
      "lakeland      lakeland        lakeland\n",
      "smoothly        smooth        smoothly\n",
      "faculty        faculti         faculty\n",
      "\n",
      "[29764 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Скачиваем необходимые ресурсы\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Инициализируем инструменты\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Обработка слов\n",
    "data = []\n",
    "for word in words:\n",
    "    stemmed = stemmer.stem(word)\n",
    "    lemmatized = lemmatizer.lemmatize(word)\n",
    "    data.append([word, stemmed, lemmatized])\n",
    "\n",
    "# Создание DataFrame\n",
    "df = pd.DataFrame(data, columns=[\"word\", \"stemmed_word\", \"normalized_word\"])\n",
    "df.set_index(\"word\", inplace=True)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2. Удалите стоп-слова из описаний рецептов. Какую долю об общего количества слов составляли стоп-слова? Сравните топ-10 самых часто употребляемых слов до и после удаления стоп-слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\khaiy/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\khaiy/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Общее количество слов: 1055765\n",
      "Осталось после удаления стоп-слов: 555605\n",
      "Доля стоп-слов: 47.37%\n",
      "\n",
      "Топ-10 слов ДО удаления стоп-слов:\n",
      "[('the', 40257), ('a', 35030), ('and', 30425), ('i', 27797), ('this', 27132), ('to', 23508), ('it', 23212), ('is', 20501), ('of', 18379), ('for', 15996)]\n",
      "\n",
      "Топ-10 слов ПОСЛЕ удаления стоп-слов:\n",
      "[('recipe', 15122), ('make', 6367), ('time', 5198), ('use', 4645), ('great', 4473), ('easy', 4206), ('like', 4186), ('one', 3916), ('good', 3847), ('made', 3819)]\n"
     ]
    }
   ],
   "source": [
    "# Скачиваем необходимые ресурсы\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Объединяем все описания в одну строку и токенизируем\n",
    "text = all_text\n",
    "tokens = word_tokenize(text.lower())  \n",
    "\n",
    "# Удаляем знаки пунктуации\n",
    "tokens = [word for word in tokens if word.isalpha()]\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "total_words = len(tokens)\n",
    "\n",
    "# Фильтруем токены — удаляем стоп-слова\n",
    "tokens_filtered = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "# Считаем оставшиеся слова\n",
    "filtered_words = len(tokens_filtered)\n",
    "\n",
    "# Доля стоп-слов\n",
    "stopword_ratio = (total_words - filtered_words) / total_words\n",
    "\n",
    "# Частотный анализ\n",
    "counter_before = Counter(tokens)\n",
    "counter_after = Counter(tokens_filtered)\n",
    "\n",
    "# Топ-10 слов до и после\n",
    "top10_before = counter_before.most_common(10)\n",
    "top10_after = counter_after.most_common(10)\n",
    "\n",
    "print(f\"Общее количество слов: {total_words}\")\n",
    "print(f\"Осталось после удаления стоп-слов: {filtered_words}\")\n",
    "print(f\"Доля стоп-слов: {stopword_ratio:.2%}\")\n",
    "print(\"\\nТоп-10 слов ДО удаления стоп-слов:\")\n",
    "print(top10_before)\n",
    "print(\"\\nТоп-10 слов ПОСЛЕ удаления стоп-слов:\")\n",
    "print(top10_after)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Векторное представление текста"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1 Выберите случайным образом 5 рецептов из набора данных. Представьте описание каждого рецепта в виде числового вектора при помощи `TfidfVectorizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0         1         2         3         4        5         6  \\\n",
      "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.00000  0.000000   \n",
      "1  0.150983  0.000000  0.101115  0.000000  0.000000  0.00000  0.000000   \n",
      "2  0.000000  0.139922  0.093707  0.000000  0.000000  0.00000  0.139922   \n",
      "3  0.000000  0.000000  0.285009  0.106393  0.106393  0.00000  0.000000   \n",
      "4  0.000000  0.000000  0.000000  0.000000  0.000000  0.57735  0.000000   \n",
      "\n",
      "          7         8         9  ...       119       120       121       122  \\\n",
      "0  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.169714   \n",
      "1  0.150983  0.000000  0.150983  ...  0.000000  0.000000  0.000000  0.000000   \n",
      "2  0.000000  0.000000  0.000000  ...  0.000000  0.279843  0.000000  0.000000   \n",
      "3  0.000000  0.106393  0.000000  ...  0.106393  0.000000  0.106393  0.000000   \n",
      "4  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
      "\n",
      "        123       124       125       126       127  \\\n",
      "0  0.000000  0.000000  0.000000  0.169714  0.000000   \n",
      "1  0.150983  0.000000  0.150983  0.000000  0.150983   \n",
      "2  0.000000  0.225776  0.000000  0.000000  0.000000   \n",
      "3  0.000000  0.171674  0.000000  0.000000  0.000000   \n",
      "4  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "\n",
      "                                original_description  \n",
      "0  this is a wonderful brunch item that tastes li...  \n",
      "1  a little different than most soda bread recipe...  \n",
      "2  we hosted hamburger barbeques on the weekends ...  \n",
      "3  so it's another night of trying to figure out ...  \n",
      "4                         authentic egyptian cookies  \n",
      "\n",
      "[5 rows x 129 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"preprocessed_descriptions.csv\")\n",
    "\n",
    "\n",
    "sample_descriptions = df[\"description\"].sample(5)\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "vector = vectorizer.fit_transform(sample_descriptions)\n",
    "\n",
    "# Преобразуем результат в DataFrame (опционально, для наглядности)\n",
    "vector_df = pd.DataFrame(vector.toarray())\n",
    "\n",
    "# Добавим описание рецепта для сопоставления\n",
    "vector_df[\"original_description\"] = sample_descriptions.values\n",
    "\n",
    "# Показываем результат\n",
    "print(vector_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2 Вычислите близость между каждой парой рецептов, выбранных в задании 3.1, используя косинусное расстояние (`scipy.spatial.distance.cosine`) Результаты оформите в виде таблицы `pd.DataFrame`. В качестве названий строк и столбцов используйте названия рецептов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Recipe 0  Recipe 1  Recipe 2  Recipe 3  Recipe 4\n",
      "Recipe 0     0.000     0.922     0.992     0.958       1.0\n",
      "Recipe 1     0.922     0.000     0.939     0.829       1.0\n",
      "Recipe 2     0.992     0.939     0.000     0.806       1.0\n",
      "Recipe 3     0.958     0.829     0.806     0.000       1.0\n",
      "Recipe 4     1.000     1.000     1.000     1.000       0.0\n"
     ]
    }
   ],
   "source": [
    "# Векторизация\n",
    "vectorizer = TfidfVectorizer()\n",
    "# descriptions = list(recipes.values())\n",
    "vector = vectorizer.fit_transform(vector_df[\"original_description\"])\n",
    "\n",
    "# Имена рецептов — используем индексы или сгенерированные имена\n",
    "names = [f\"Recipe {i}\" for i in vector_df.index]\n",
    "\n",
    "# Создаём DataFrame для хранения расстояний\n",
    "n = len(names)\n",
    "dist_matrix = pd.DataFrame(index=names, columns=names, dtype=float)\n",
    "\n",
    "# Заполняем матрицу косинусных расстояний\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        vec_i = vector[i].toarray()[0]\n",
    "        vec_j = vector[j].toarray()[0]\n",
    "        dist = cosine(vec_i, vec_j)\n",
    "        dist_matrix.iloc[i, j] = dist\n",
    "\n",
    "print(dist_matrix.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.3 Какие рецепты являются наиболее похожими? Прокомментируйте результат (словами)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Наиболее похожие рецепты: we hosted hamburger barbeques  и so it's another night of tryin\n",
      "Косинусное расстояние между ними: 0.806\n"
     ]
    }
   ],
   "source": [
    "# Найдём индексы минимального ненулевого расстояния\n",
    "min_dist = np.inf\n",
    "closest_pair = (None, None)\n",
    "\n",
    "for i in range(len(names)):\n",
    "    for j in range(i + 1, len(names)):\n",
    "        dist = dist_matrix.iloc[i, j]\n",
    "        if dist < min_dist:\n",
    "            min_dist = dist\n",
    "            closest_pair = (names[i], names[j])\n",
    "\n",
    "print(f\"Наиболее похожие рецепты: {closest_pair[0]} и {closest_pair[1]}\")\n",
    "print(f\"Косинусное расстояние между ними: {min_dist:.3f}\")\n",
    "\n",
    "# Косинусное расстояние между их описаниями составляет 0.806, что говорит о невысокой степени текстового сходства.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
